Dezember 2010, am Südpol. Gut gelaunt zählt die Crew, vermummt in dicke Klamotten, den Countdown herunter. Bei "null" verschwindet eine basketballgroße Kugel in einem tiefen Loch, gebohrt ins Eis der Antarktis. Es ist der letzte von gut 5000 Spezialsensoren des Teilchendetektors IceCube.
"Die sind unter der Eisoberfläche quasi festgefroren", sagt Tim Ruhe, Physiker an der TU Dortmund: "In einer Tiefe von ungefähr 1,5 Kilometern beginnen die und reichen dann runter bis 2,5 Kilometer. Und die messen Licht, was in Teilchen-Interaktionen entsteht."
Diese Teilchen-Interaktionen betreffen sogenannte Neutrinos. Das sind geisterhafte Teilchen, die durchs All rasen und zum Beispiel bei kosmischen Gewaltakten entstehen. Ab und zu stößt ein solches Neutrino mit einem Atomkern im antarktischen Eis zusammen und ruft ein schwaches Leuchten hervor. Das Leuchten fangen die Sensorkugeln auf und weisen dadurch die Neutrinos nach. Diese fungieren als Botenteilchen aus den Tiefen des Alls und verraten neue Details, etwa über schwarze Löcher. Das Problem: Es gibt auch andere Teilchen, die im Detektor ein Leuchten hervorrufen. Tim Ruhe:
"Für die Neutrino-Detektoren sind das tatsächlich Störsignale. Die müssen wir halt in irgendeiner Art und Weise herausfiltern bzw. erkennen. Und dabei haben wir unter anderem maschinelles Lernen eingesetzt."
Die Herausforderungen beim Herausfischen der Signale sind groß. Denn bei den Analysen muss man bis zu 60 Variablen gleichzeitig im Blick haben. Mathematisch gesehen bedeutet das: Man bewegt sich in 60 Dimensionen.
"In 60 Dimensionen kann ich als Mensch nicht denken. Ein Computeralgorithmus kann das aber sehr gut und findet dann einfach mehr Neutrinos in der gleichen Menge an Grunddaten, als dass ein Mensch könnte. Ein Computer kann das sehr viel besser, sehr viel effizienter."
Konkret haben die Dortmunder Physiker ein neuronales Netz programmiert – einen Algorithmus, der der Funktionsweise unseres Gehirns nachempfunden ist. Trainiert mit Hilfe von riesigen Datensätzen lernt er von selbst, wie sich ein Neutrino-Ereignis von einem Störsignal unterscheidet, indem er in den bis zu 60 mathematischen Dimensionen nach bestimmten Mustern sucht. Immer wieder kontrollieren die Forscher die Resultate der KI und teilen ihr mit, ob sie richtig liegt oder nicht. Je mehr Datensätze der Algorithmus durchforstet, um so besser werden seine Ergebnisse.
"Wozu die Algorithmen beitragen, dass wir wesentlich mehr Neutrinos sehen können. Mehr Neutrinos bedeutet kleinere statistische Fehler, also kleinere Unsicherheiten."
Der Algorithmus erkennt also Ereignisse, die den Forschern sonst schlicht durch die Lappen gehen würden. Jetzt ist Ruhes Team dabei, der KI noch mehr beizubringen –eine verbesserte Rekonstruktion der Neutrino-Flugrichtung.
"Eine verbesserte Rekonstruktion heißt, dass wir sehr viel genauer wissen können, woher die Neutrinos kommen, wobei wir das noch nicht wirklich auf die Daten angewendet haben. Das entwickelt wir noch."
Und das Team tüftelt bereits daran, die Methode auf andere Großgeräte zu übertragen.
"Das Schöne an diesen maschinellen Lernmethoden ist, dass die Grundkonzepte eigentlich immer die gleichen bleiben. Da ist dann eine gewisse Übertragbarkeit gegeben. Im Moment strecken unsere Fühler in Richtung Radioastronomie aus, und auch Gamma-Astronomie, weil da noch viel größere Datenmengen anfallen werden."
Konkret geht es um SKA, ein riesiges Observatorium für Radiowellen, und um das künftige Gamma-Teleskop CTA. Beide könnten, so hofft Tim Ruhe, sogar noch stärker von den lernfähigen Algorithmen profitieren als IceCube, der Neutrino-Detektor im antarktischen Eis.